import torch
from threading import Thread

import gradio as gr
import spaces
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from datetime import datetime

model_path = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
#model_path = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

title="Chat with DeepSeek Model"

DEFAULT_SYSTEM = "You are a helpful assistant.Always enclose latex snippets with dollar signs! For example, $$\phi$$"

model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype="auto", device_map="cuda")
tokenizer = AutoTokenizer.from_pretrained(model_path)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id


@spaces.GPU
def stream_chat(message: str, history: list, system: str, temperature: float, max_new_tokens: int):
    msg_id = int(datetime.now().timestamp() * 1000)
    print(msg_id)
    conversation = [{"role": "system", "content": system or DEFAULT_SYSTEM}]
    #for h in history:
    #    conversation.extend([{"role": h['role'], "content": h['content']}])

    conversation.append({"role": "user", "content": message})
    print(conversation)

    inputs = tokenizer.apply_chat_template(conversation,
            return_tensors="pt",
            ).to(model.device)
    print(f"inputs: {type(inputs)}")
    streamer = TextIteratorStreamer(tokenizer, timeout=6*10.0, skip_prompt=True, skip_special_tokens=True)

    generate_kwargs = dict(
        input_ids=inputs,
        pad_token_id=tokenizer.pad_token_id,
        streamer=streamer,
        #max_new_tokens=max_new_tokens,
        max_new_tokens=16384,
        temperature=temperature,
        do_sample=True,
    )
    if temperature == 0:
        generate_kwargs["do_sample"] = False

    t = Thread(target=model.generate, kwargs=generate_kwargs)
    t.start()

    history.append(dict(role='assistant',
                    content='',
                    metadata={
                        'title':'ğŸ’­',
                        'log':f'æ­£åœ¨æ€è€ƒ{message}',
                        'status':'pending',
                        }))

    state = 1 # 1 means 'thinking' and 2 means 'answering'

    for new_token in streamer:
        if new_token == '<think>':
            state = 1
        if new_token.find('</think>') != -1:
            history[-1]['metadata']['log'] = ''
            history[-1]['metadata']['status'] = 'done'
            history[-1]['metadata']['parent_id'] = msg_id
            #history.append({'role':'system',
            #                'content':DEFAULT_SYSTEM}
            #                )

            history.append(dict(role='assistant',
                                content='',
                                metadata={'id':msg_id}))
            state = 2
            yield history[-2:]
        else:
            pass
        if state == 1:
            print(new_token, end='')
            history[-1]['content'] += new_token
            yield history[-1:]
        elif state == 2:
            #print(new_token, end='')
            history[-1]['content'] += new_token
            yield history[-2:]
            #yield history[-1]['content']
        else:
            continue

chatbot = gr.Chatbot(type='messages')

with gr.Blocks(
        title="å¦‚ä½•å°†CUDAç¯å¢ƒä¸‹çš„AIæ¨¡å‹å¹³æ»‘è¿ç§»è‡³MUSAè½¯ä»¶æ ˆ",
        ) as demo:
    gr.ChatInterface(
        title="å¦‚ä½•å°†CUDAç¯å¢ƒä¸‹çš„AIæ¨¡å‹å¹³æ»‘è¿ç§»è‡³MUSAè½¯ä»¶æ ˆ",
        description='''æ‘©å°”çº¿ç¨‹ï¼ˆMoore Threadsï¼‰æ¨å‡ºçš„å…¨åŠŸèƒ½GPUå’ŒMUSAè½¯ä»¶æ ˆä¸ºå¼€å‘è€…æä¾›äº†å¼ºå¤§çš„æ”¯æŒï¼Œå‡­å€Ÿå…¶å‡ºè‰²çš„å…¼å®¹æ€§å’Œç®€ä¾¿çš„è¿ç§»æ–¹å¼ï¼Œ å¼€å‘è€…å¯ä»¥å®ç°æ— ç¼è¿‡æ¸¡ã€‚ä»¥ä¸‹æ˜¯å°†ä¸€ä¸ªåœ¨CUDAç¯å¢ƒä¸‹è¿è¡Œçš„AIæ¨¡å‹ï¼ˆDeepSeek R1 Distill Qwen 1.5Bï¼‰å¹³æ»‘è¿ç§»è‡³MUSAè½¯ä»¶æ ˆçš„å…·ä½“æ­¥éª¤ï¼š
#### æ­¥éª¤ 1ï¼šåŠ å…¥æ‘©å°”çº¿ç¨‹ Torch MUSA æ’ä»¶
1. é¦–å…ˆï¼Œç¡®è®¤ä½ çš„å¼€å‘ç¯å¢ƒä¸­å®‰è£…äº†æœ€æ–°ç‰ˆæœ¬çš„æ‘©å°”çº¿ç¨‹ Torch MUSA æ’ä»¶ã€‚å¯ä»¥è®¿é—®æ‘©å°”çº¿ç¨‹çš„å®˜æ–¹ç½‘ç«™æˆ–å…¶GitHubé¡µé¢ï¼Œä¸‹è½½å¹¶å®‰è£…æ’ä»¶ã€‚
2. å®‰è£…å®Œæˆåï¼Œä½¿ç”¨`pip`æˆ–å…¶ä»–åŒ…ç®¡ç†å·¥å…·ç¡®è®¤æ’ä»¶å·²æˆåŠŸé›†æˆåˆ°ä½ çš„Pythonç¯å¢ƒä¸­ã€‚
#### æ­¥éª¤ 2ï¼šæ›¿æ¢å·²æœ‰ä»£ç ä¸­çš„CUDAå­—æ ·ä¸ºMUSA
1. æ‰“å¼€DeepSeek R1 Distill Qwen 1.5Bçš„æºä»£ç ï¼Œé€ä¸€æœç´¢æ‰€æœ‰æ¶‰åŠåˆ°CUDAçš„ç›¸å…³è°ƒç”¨ï¼ˆå¦‚`torch.cuda`ã€`cudaMalloc`ç­‰ï¼‰ã€‚
2. å°†è¿™äº›CUDAå‡½æ•°çš„è°ƒç”¨æ›¿æ¢ä¸ºMUSAç›¸åº”çš„å‡½æ•°æˆ–æ–¹æ³•ï¼Œå¦‚å°†`cuda`æ›¿æ¢ä¸º`musa`ã€‚
3. ç¡®ä¿æ‰€æœ‰æ•°æ®ä¼ è¾“ã€å¼ é‡æ“ä½œå’Œè®¾å¤‡ç®¡ç†éƒ½å·²æ­£ç¡®æ›´æ–°ï¼Œç¡®ä¿ä¸MUSAè½¯ä»¶æ ˆçš„å…¼å®¹æ€§ã€‚
#### æ­¥éª¤ 3ï¼šè¿è¡Œç¤ºä¾‹
1. åœ¨å®Œæˆæ‰€æœ‰ä¿®æ”¹åï¼Œå¯¹ä»£ç è¿›è¡Œå…¨é¢æ£€æŸ¥ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼ä»»ä½•CUDAç›¸å…³çš„å†…å®¹ã€‚
2. è¿è¡Œä¿®æ”¹åçš„æ¨¡å‹ï¼Œè§‚å¯Ÿæ˜¯å¦èƒ½å¤ŸæˆåŠŸåœ¨MUSAç¯å¢ƒä¸­å¯åŠ¨å’Œæ‰§è¡Œã€‚å¦‚æœé‡åˆ°ä»»ä½•é”™è¯¯ï¼Œè¯·ä»”ç»†é˜…è¯»é”™è¯¯ä¿¡æ¯ï¼Œæ’æŸ¥å…¼å®¹æ€§é—®é¢˜ã€‚
3. æœ€åï¼Œé€šè¿‡å¯¹æ¯”åœ¨CUDAå’ŒMUSAç¯å¢ƒä¸‹çš„æ‰§è¡Œç»“æœå’Œæ€§èƒ½ï¼Œç¡®ä¿è¿ç§»çš„æˆåŠŸã€‚
é€šè¿‡éµå¾ªä¸Šè¿°æ­¥éª¤ï¼Œå¼€å‘è€…å¯ä»¥å°†DeepSeek R1 Distill Qwen 1.5Bæ¨¡å‹å¹³æ»‘åœ°è¿ç§»è‡³MUSAè½¯ä»¶æ ˆï¼Œä»è€Œå……åˆ†åˆ©ç”¨æ‘©å°”çº¿ç¨‹çš„å…¨åŠŸèƒ½GPUå¸¦æ¥çš„æ€§èƒ½ä¼˜åŠ¿ã€‚è¿™æ ·çš„è¿ç§»ä¸ä»…èƒ½æé«˜å¼€å‘æ•ˆç‡ï¼Œè¿˜èƒ½æœ‰æ•ˆé™ä½æŠ€æœ¯è½¬å‹çš„æˆæœ¬ã€‚''',
        type='messages',
        fn=stream_chat,
        chatbot=chatbot,
        fill_height=True,
        additional_inputs_accordion=gr.Accordion(label="âš™ï¸ Parameters", open=False, render=False),
        additional_inputs=[
            gr.Text(
                value="",
                label="System",
                render=False,
            ),
            gr.Slider(
                minimum=0,
                maximum=1,
                step=0.1,
                value=0.8,
                label="Temperature",
                render=False,
            ),
            gr.Slider(
                minimum=128,
                maximum=4096,
                step=1,
                value=1024,
                label="Max new tokens",
                render=False,
            ),
        ],
        examples=[
            ["è§£æ–¹ç¨‹ï¼š3x + 5 = 20ã€‚æ±‚ x çš„å€¼ã€‚", ""],
            ["ä¸€ä¸ªçŸ©å½¢çš„é•¿æ˜¯8å˜ç±³ï¼Œå®½æ˜¯5å˜ç±³ã€‚è¯·è®¡ç®—è¯¥çŸ©å½¢çš„å‘¨é•¿å’Œé¢ç§¯ã€‚", ""],
            ["è®¡ç®—åœ†é”¥ç»•å…¶åº•é¢ç›´å¾„æ—‹è½¬ (180^\circ) å½¢æˆçš„ç«‹ä½“ä½“ç§¯ã€‚", ""],
            ["è®¾å‡½æ•° f(x) = 2x + 3ã€‚æ±‚ f(4) çš„å€¼ã€‚", ""],
            ["ä¸€ä¸ªè¢‹å­é‡Œæœ‰5ä¸ªçº¢çƒå’Œ3ä¸ªè“çƒã€‚å¦‚æœéšæœºæŠ½å–ä¸€ä¸ªçƒï¼ŒæŠ½åˆ°çº¢çƒçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ", ""],
            ["âˆ«(x^2 + 2x) dxï¼ŒåŒºé—´ä¸ºä»1åˆ°4ã€‚è¯·ç»™å‡ºç»“æœï¼Œå¹¶è§£é‡Šä½ çš„è¿‡ç¨‹ã€‚", ""],
            ["ç”¨C++å®ç°KMPç®—æ³•ï¼Œå¹¶åŠ ä¸Šä¸­æ–‡æ³¨é‡Š", ""],
        ],
        cache_examples=False,
    )


if __name__ == "__main__":
    demo.launch(server_name = '0.0.0.0')
